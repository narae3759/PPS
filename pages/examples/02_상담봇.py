import streamlit as st
from custom_functions import *
	
# llm ìƒì„±
from langchain import hub 
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

load_style()
session_key = "chat_history"
###########################################################################
# Page ì‹œì‘
###########################################################################
# Insert Text
st.markdown("""
            <div class="info-container">
            ğŸ“¢ ê¸°ëŠ¥ ì„¤ëª…(ì‘ì—… ì¤‘)
            <li> ì‚¬ìš©ìì˜ ì‹¬ë¦¬ìƒë‹´ì„ í•´ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. </li>
            <li> ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì—¬ ëŒ€ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. </li>
            <li> ìƒë‹´ì´ ì•„ë‹Œ íŠ¹ì • ì—…ë¬´ì— ëŒ€í•œ ì±—ë´‡ìœ¼ë¡œ ë°”ê¿€ ì˜ˆì •ì…ë‹ˆë‹¤. </li>
            </div>
            """, unsafe_allow_html=True)
#--------------------------------------------------------------------------
## Settings
#--------------------------------------------------------------------------
if session_key not in st.session_state:
    st.session_state[session_key] = []

if "counseling_chain" in st.session_state:
    chat = st.session_state["counseling_chain"]
    memory = st.session_state["counseling_memory"]
else:
    # Chat ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    # prompt = PromptTemplate.from_template(template)
    memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")
    runnable = RunnablePassthrough.assign(
        chat_history = RunnableLambda(memory.load_memory_variables)
        | itemgetter("chat_history")
    )
    prompt = hub.pull("thinker/counseling_korean")
    model = ChatOpenAI(model_name="gpt-4o",streaming=True)
    chain = runnable | prompt | model | StrOutputParser()

    chat = Chat(chain, session_key)
    st.session_state["counseling_chain"] = chat
    st.session_state["counseling_memory"] = memory

#--------------------------------------------------------------------------
## Header & Body
#--------------------------------------------------------------------------

# ì²« ì±„íŒ…ì„ ì‹œì‘í•  ë•Œ ì²« ì¸ì‚¬ ì¶œë ¥
if len(st.session_state[session_key]) == 0:
    greeting = "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê³ ë¯¼ì„ ë“¤ì–´ë“œë¦´ê²Œìš”ğŸ˜Š"
    chat.greeting(greeting)
# ì±„íŒ… ê¸°ë¡ì´ ìˆì„ ë•Œ ê¸°ë¡ëœ ì±„íŒ… ì¶œë ¥
else:
    for history in st.session_state[session_key]:
        st.chat_message(history["role"]).markdown(history["content"])

question = st.chat_input(placeholder="ë©”ì„¸ì§€ ì…ë ¥")

# ì±„íŒ…ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ
if question:
    # ì…ë ¥ëœ ì±„íŒ… ì¶œë ¥
    chat.input_user(question)

    # ë‹µë³€ ì¶œë ¥
    answer = chat.input_assistant(question)
    # ë©”ëª¨ë¦¬ ì €ì¥
    memory.save_context(
        {"inputs": question},
        {"output": answer}
    )
    # ë©”ëª¨ë¦¬ ì¶œë ¥
    print(memory.load_memory_variables({}))
